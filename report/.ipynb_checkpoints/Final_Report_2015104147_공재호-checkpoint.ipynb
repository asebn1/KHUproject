{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 경희대학교 컴퓨터공학과 공지사항 및 취업정보 분석(2015104147 공재호)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 주제 선정 이유<br>\n",
    " 텀 프로젝트 주제는 경희대학교 컴퓨터공학과 공지사항 및 취업 정보 분석이다. 경희대학교 학부 학생들은 학교의 공지사항 및 취업 정보를 알고 싶다. 공지사항이 경희톡으로 오기도 하지만 보편화한 지 얼마 되지 않았다. 또한, 알고 싶은 정보가 오지 않을 때도 많기 때문에 직접 찾아보아야 한다. 어떤 기업의 취업 정보가 많은지와 공지사항에서는 달마다 어떤 내용이 공지되는지 알 수 있도록 한다. 이전 2015년부터 현재까지의 공지사항 및 취업 정보를 바탕으로 정보를 분석하기 위해 이 주제를 선정하였다.<br>\n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.가설 정의<br>\n",
    " 2015년부터 2019년까지 매년 9월마다 가을 프로그래밍 경시대회가 공지된다고 가정하자. 그렇다면, 2020년 9월에도 가을 프로그래밍 경시대회가 공지된다는 것을 유추할 수 있다. 이처럼 주기적으로 공지되는 타이틀 정보를 저장하여 분석한다. 분석된 자료를 통해 앞으로 공지될 공지사항 및 취업 정보를 예측할 수 있다.<br>\n",
    " <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.인터넷을 통한 데이터 획득<br>\n",
    "1. 경희대학교 SW융합대학 - 컴퓨터공학과 - 공지사항<br>\n",
    " 공지사항은 등록일을 기준으로 하며 번호와 공지 제목이 있다. 등록일과 공지 제목을 통해 달마다 어떤 내용이 공지되는지 분석하여 확인할 수 있다.<br>\n",
    "2. 경희대학교 SW융합대학 - 컴퓨터공학과 - 취업<br>\n",
    " 취업 정보는 등록일을 기준으로 하며 번호와 제목이 있다. 등록일과 제목을 통해 달마다 어떤 기업의 취업 정보가 공지되는지 분석하여 확인할 수 있다.<br>\n",
    " <br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.분석을 위한 데이터의 가공<br>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<데이터를 수집하고 분석하기 위한 사전 작업><br>\n",
    "\n",
    " 1. beautifulSoup4를 설치 (pip install beautifulSoup4)\n",
    " 2. url을 통해 file을 open하기 위한 urllib 설치 (pip install urllib)\n",
    " 3. 크롤링 데이터를 csv 파일로 저장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 공지사항의 정보들을 추출한다. 번호, 제목, 등록일자를 추출한다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▲ 페이지를 count하여 페이지 끝까지 정보 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▲ 공지사항 정보 추출 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def getNotice():\n",
    "    #총 페이지\n",
    "    count = 42\n",
    "\n",
    "    #홍페이지 주소. 페이지 마다 count 해주어 게시판 글 모두 긁어옴\n",
    "    baseurl = 'http://ce.khu.ac.kr/index.php?pg=' + str(count) + '&page=list&hCode=BOARD&bo_idx=2&sfl=&stx='\n",
    "    res = req.urlopen(baseurl)\n",
    "    soup = BeautifulSoup(res, 'html.parser')\n",
    "    \n",
    "    # 호환되도록 버퍼 -1과 utf8 적용\n",
    "    f = open('Notice.csv', 'a', -1, \"utf-8\")\n",
    "\n",
    "    #페이지가 끝날때까지 반복\n",
    "    while(1):\n",
    "        baseurl = 'http://ce.khu.ac.kr/index.php?pg=' + str(count) + '&page=list&hCode=BOARD&bo_idx=2&sfl=&stx='\n",
    "        res = req.urlopen(baseurl)\n",
    "        soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "        if count == 42:\n",
    "\n",
    "            #마지막 페이지일 때 게시글 수가 20개가 아니므로\n",
    "            for i in range(12, 1, -1):\n",
    "                #번호 삽입\n",
    "                temp1 = '#board_list >tbody > tr:nth-child(' + str(i) + ') > td:nth-child(1)'\n",
    "                noticeNum = soup.select(temp1)[0].text\n",
    "                #제목 삽입\n",
    "                temp2 = '#board_list > tbody > tr:nth-child(' + str(i) + ') > td:nth-child(2) > a'\n",
    "                noticeText = soup.select(temp2)[0].text\n",
    "                #등록일 삽입\n",
    "                temp3 = '#board_list > tbody > tr:nth-child(' + str(i) + ') > td:nth-child(5)'\n",
    "                noticeym = soup.select(temp3)[0].text\n",
    "                print(noticeNum + ',' + noticeText  + ',' + noticeym)\n",
    "                f.write(noticeNum + ',' + noticeText  + ',' + noticeym + '\\n')\n",
    "       \n",
    "        else :\n",
    "            #기본 페이지와 같은 수일때 적용\n",
    "            for i in range(21, 1, -1):\n",
    "                #번호 삽입\n",
    "                temp1 = '#board_list >tbody > tr:nth-child(' + str(i) + ') > td:nth-child(1)'\n",
    "                noticeNum = soup.select(temp1)[0].text\n",
    "                #제목 삽입\n",
    "                temp2 = '#board_list > tbody > tr:nth-child(' + str(i) + ') > td:nth-child(2) > a'\n",
    "                noticeText = soup.select(temp2)[0].text\n",
    "                noticeText = noticeText.replace(',','')\n",
    "                \n",
    "                #등록일 삽입\n",
    "                temp3 = '#board_list > tbody > tr:nth-child(' + str(i) + ') > td:nth-child(5)'\n",
    "                noticeym = soup.select(temp3)[0].text\n",
    "                print(noticeNum + ',' + noticeText  + ',' + noticeym)\n",
    "                f.write(noticeNum + ',' + noticeText  + ',' + noticeym + '\\n')\n",
    "        count -= 1\n",
    "\n",
    "        #게시물 끝 도달시 종료\n",
    "        if count == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 취업정보란의 정보들을 추출한다. 번호, 제목, 등록일자를 추출한다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▲ 페이지를 count하여 페이지 끝까지 정보 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▲ 취업정보란의 정보 추출 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#홈페이지에서 크롤링으로 값 받아오기\n",
    "def getEmployment():\n",
    "\n",
    "    #총 페이지\n",
    "    count = 17\n",
    "\n",
    "    #홍페이지 주소. 페이지 마다 count 해주어 게시판 글 모두 긁어옴\n",
    "    baseurl = 'http://ce.khu.ac.kr/index.php?pg=' + str(count) + '&page=list&hCode=BOARD&bo_idx=3&sfl=&stx='\n",
    "    res = req.urlopen(baseurl)\n",
    "    soup = BeautifulSoup(res, 'html.parser')\n",
    "    \n",
    "    # 호환되도록 버퍼 -1과 utf8 적용\n",
    "    f = open('Employment.csv', 'a', -1, \"utf-8\")\n",
    "    \n",
    "    #페이지가 끝날때까지 반복\n",
    "    while(1):\n",
    "        baseurl = 'http://ce.khu.ac.kr/index.php?pg=' + str(count) + '&page=list&hCode=BOARD&bo_idx=3&sfl=&stx='\n",
    "        res = req.urlopen(baseurl)\n",
    "        soup = BeautifulSoup(res, 'html.parser')\n",
    "        \n",
    "        #마지막 페이지일 때 게시글 수가 20개가 아니므로\n",
    "        if count == 17:\n",
    "            for i in range(19, 0, -1):\n",
    "                #번호 삽입\n",
    "                temp1 = '#board_list >tbody > tr:nth-child(' + str(i) + ') > td:nth-child(1)'\n",
    "                noticeNum = soup.select(temp1)[0].text\n",
    "                #제목 삽입\n",
    "                temp2 = '#board_list > tbody > tr:nth-child(' + str(i) + ') > td:nth-child(2) > a'\n",
    "                noticeText = soup.select(temp2)[0].text\n",
    "                #등록일 삽입\n",
    "                temp3 = '#board_list > tbody > tr:nth-child(' + str(i) + ') > td:nth-child(5)'\n",
    "                noticeym = soup.select(temp3)[0].text\n",
    "                print(noticeNum + ',' + noticeText  + ',' + noticeym)\n",
    "                f.write(noticeNum + ',' + noticeText  + ',' + noticeym + '\\n')\n",
    "   \n",
    "        else :\n",
    "            #기본 페이지와 같은 수일때 적용\n",
    "            for i in range(20, 0, -1):\n",
    "                #번호 삽입\n",
    "                temp1 = '#board_list >tbody > tr:nth-child(' + str(i) + ') > td:nth-child(1)'\n",
    "                noticeNum = soup.select(temp1)[0].text\n",
    "                #제목 삽입\n",
    "                temp2 = '#board_list > tbody > tr:nth-child(' + str(i) + ') > td:nth-child(2) > a'\n",
    "                noticeText = soup.select(temp2)[0].text\n",
    "                noticeText = noticeText.replace(',','')\n",
    "                \n",
    "                #등록일 삽입\n",
    "                temp3 = '#board_list > tbody > tr:nth-child(' + str(i) + ') > td:nth-child(5)'\n",
    "                noticeym = soup.select(temp3)[0].text\n",
    "                print(noticeNum + ',' + noticeText  + ',' + noticeym)\n",
    "                f.write(noticeNum + ',' + noticeText  + ',' + noticeym + '\\n')\n",
    "        count -= 1\n",
    "       \n",
    "        #게시물 끝 도달시 종료\n",
    "        if count == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 크롤링을 통해 저장된 csv의 정보를 활용하여 앞으로 나올 공지사항과 취업정보를 예상한다.\n",
    "예시)\n",
    "1-1) 2016년부터 2019년까지 매년 9월 혹은 10월에 KHU 가을 프로그래밍 경시대회 안내가 나오는 것을 확인한다.<br>\n",
    "1-2) 2020년에도 9월 혹은 10월에 KHU 가을 프로그래밍 경시대회가 나올 수 있음을 예상할 수 있다.<br>\n",
    "\n",
    "2-1) 2015년부터 2019년까지 매년 4월 삼성 취업설명회 안내가 나오는 것을 취업정보란을 통해 확인한다.<br>\n",
    "2-2) 2020년에도 4월에 삼성 취업설명회 안내가 나올 수 있음을 예상할 수 있다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 앞선 과정을 통해 얻은 데이터를 이용하여 앞으로 나올 공지사항 혹은 취업 정보를 예상한다.\n",
    "[3]의 과정을 통해 경희대학교 컴퓨터공학과와 취업정보란을 통해 앞으로 나올 정보를 예상할 수 있다.<br>\n",
    "구한 csv 파일의 [제목]란을 통해 알아보고자 하는 정보를 string형식으로 입력해 정보를 찾는다. <br>\n",
    "<br>\n",
    "또한, 검색된 결과에서 등록일을 확인한다. 등록일 사이에 얼만큼의 주기를 가지고 규칙이 있는지 확인한다.<br>\n",
    "만약 규칙이 있다면, 다음 2020년 이후에도 같은 주기로 비슷한 내용의 공지가 올라올 수 있음을 예상한다. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. 분석 결과 도출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "컴퓨터공학과의 공지사항이 시작된 2015-06-30 부터 2020-05-30까지를 기준으로 공지사항 내용이 수집되었다. <br>\n",
    "컴퓨터공학과 취업정보란 시작된 2015-06-30 부터 2020-05-30까지를 기준으로 취업정보가 수집되었다.<br>\n",
    "\n",
    "1) Notice : 공지사항 <br>\n",
    "2) Employment : 취업정보 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) 획득 데이터 csv는 다음과 같이 저장된다.\n",
    "<br>\n",
    "[공지사항]   notice.csv 파일로 저장<br><br>\n",
    "    번호,                제목,                               등록일<br>\n",
    "    1      2015-1학기 졸업능력인증 신청접수 안내         2015-06-30<br>\n",
    "    2  [LINC+] 2015학년도 1학기 경희대학교 창업동아리    2015-07-18<br>\n",
    "    3  [LINC+] Microsoft Learning Program(MLP) 교육과정  2015-07-18<br>\n",
    "    4   [중요] 전자정보대학관내 마스크 필수 착용 협조    2015-07-20<br>\n",
    "    5   [중요] 이태원 클럽 방문 내/ 외국인 검역 협조     2015-08-15<br>\n",
    "    6   2015-1학기 국제 생활관(우정원, 제2기숙사) 입사   2015-08-17<br>\n",
    "    <br>\n",
    "     .<br>\n",
    "     .<br>\n",
    "     .<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. 데이터 분석 결과 및 결론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "▲ 경희대학교 컴퓨터공학과의 공지사항 및 취업 정보 데이터를 추출하고 가공한다. 정보를 분석하여 앞으로 공지될 공지사항 및 취업 정보를 예측하고 달마다 공지되는 양을 파악한다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. 참고문헌 및 자료"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경희대학교 컴퓨터공학과 공지사항:http://ce.khu.ac.kr/index.php?hCode=BOARD&bo_idx=2 <br>\n",
    "경희대학교 컴퓨터공학과 취업정보:http://ce.khu.ac.kr/index.php?hCode=BOARD&bo_idx=3<br>\n",
    "BeautifulSoup를 활용: https://freeharmony.tistory.com/64 <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. 파이썬 소스코드 및 수집 데이터 (별첨)<br>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
